Reviewer #1 (Comments to the Author):

I thank to the authors submitting the revised version of this work. I am heartened that the authors accept the critiques in the spirit of improving this work. The authors complemented some of the weaknesses that are mentioned by the reviewers.

This paper tries to discover gender biases in Google Translate, by translating the list of job positions from the U.S. Bureau of Labor Statistics (BLS), that are written in 11 different gender-neutral languages, into English, which is considered to be a lingua franca. Assuming that a translation tool already reflected the inequality existing in a society, the authors further compared the distribution of gender pronouns in their experiments to the gender distribution of each occupation in US. What they are found is somewhat expected but meaningful. Google Translate tends to result in male defaults. They also argued that Google Translate is more likely to yield male defaults than what would be expected from demographic data alone.

This work may add empirical evidences on how a machine, which is trained by a black-box algorithm owned by a corporation, is biased and call for future endeavors on correcting this, including certain approaches such as debiasing techniques suggested in a line of studies, which is also mentioned in this paper.

Yet, after carefully reading the paper several times, I found it has certain weaknesses on its methodology that degrade the scientific value of this paper.

First, the analysis scheme used in this paper is vulnerable to the selection of the languages used for translations. As presented in Section 6, languages have different tendencies in gender-related pronouns in the translated outputs. However, the authors just counted the frequency of male (or female) related pronouns without a proper normalization. According to the Simpsonâ€™s paradox (https://en.wikipedia.org/wiki/Simpson%27s_paradox), the aggregated data may not represent the real trend that can be observed in each group.

Moreover, it is unclear how the authors selected the languages used for translation. In Table 1, The authors mentioned that they work only with languages that lack a pronominal gender system and are supported by Google Translate. Then, why are Bengali and Nepali that satisfy both conditions not included in the analysis? In addition, why were Korean excluded with the characteristic same as Chinese? Again, the analysis scheme of this paper is highly volatile to the selection of languages, and hence the missing rationale of language selection degrades the reliability of the finding.

It is also unclear how they conducted the translation experiment with adjectives. Among numerous adjectives in English, how did the authors select the list of adjectives presented in Table 5? Did they use the templates similar to Table 3 for adjectives? If so, where is the template?

Comparison with women participation data across job position (Section  does not make sense. For each occupation, the authors translated each language into English and measured the fraction of female pronouns out of the number of language. Such fraction was compared with the real-world female proportion corresponding to each occupation in US. Why should be the fraction of female pronouns that are based on translation results from different languages similar to the real-world female proportion for an occupation?

Rather than the fraction of female pronouns, I guess the statistical confidence with regard on translating into female pronouns could be related to the female proportion for an occupation. But Google Translate only returns a translated result without any statistical confidence or probability. Thus, it may be very difficult to make such comparisons with publicly available data.

I strongly believe that this paper requires a substantial amount of editing to be readable. While I am satisfied with the writing of Abstract and Introduction, the rest of the paper misses important details that are required to follow their claims. The presentation of the figures also requires significant improvements. Figure 2-7 are really confusing. Such figures could be better presented with empirical cdfs on selected categories (e.g., top 10 in terms of frequency). Figure 8 is required to have certain spaces between bars corresponding to each value along the x-axis. The figure also contains confusing labels corresponding to each color (i.e., GT F/A, GT F/(M+F), BLS data).

For the aforementioned reasons, I think this paper is not ready to be published in Palgrave Communications for now. Yet, I believe it is crucial for research community to work on the machine bias problem to make our society a better place. I hope to see the improved version of this work in a possible revision or another venue.